					Road map
This course is divided into four parts each of wich discusses
different themes, paradigms and algorithms with difficulty in ascendant order.

	01_chap "Introduction":

Part 1 "Fundamental concepts"

	02_chap "":
	
	03_chap "":

	04_chap "Compute Architecture and Scheduling":
	Consolidates discussions on architecture and scheduling considerations.

	05_chap "Memory Architecture and Data Locality":

	06_chap "Performance Considerations":
	Provides a more thorough handling of thread granularity considerations and
	checklist of common performance optimisation strategies and the performance
	bottlenecks that each strategy tackles.

Part 2 "Parallel Patterns"

	07_chap "Convolution":
	Unlike the previous edition, the current one focuses more on two-dimensional
	convolution.

	08_chap "Stencil":
	Provides a more thorough treatment of the 'stencil' pattern, emphasizing the
	mathematical backgroung behind the computation and aspects that make it
	different from convolution, thereby enabling additional optimizations.

	09_chap "Parallel Histogram":
	Applies incremental approach to performance optimization of 'histogram' pattern.
	The first optimization to be applied is 'Privatization', which is aimed to reduce
	contention of atomics.
	The second one is 'Thread coarsening' to reduce the number of private copies
	committed to the public copy.
	The third one is 'Shared memory for the private bins' to reduce the access
	latency. 

	10_chap "Reduction and Minimizing Divergence":
	Provides a more complete presentation of the 'reduction' pattern with an
	incremental approach to applying the optimizations and a more thorough analysis
	of the associated performance tradeoffs.

	11_chap "Prefix sum":
	Important algorithm primitive that can facilitate the conversion of sequential,
	recursive formulation of the problems into more parallel forms.

	12_chap "":

Part 3 "Advanced Patterns and Applications"

	13_chap "Sorting":
	Presents 'radix sort' as a noncomparison sort algorithm that is highly
	amendable to GPU parallelization and follows an incremental approach to optimizing
	it and analyzing the performance tradeoffs.

	14_chap "Sparse Matrix Computation":
	Introduces a list of considerations that go into the design of different
	'sparse matrix storage' formats which is used throughout this book to analyze the
	tradeoffs between the different formats.

	15_chap "Graph Traversal":
	Covers a more comprehensive set of parallelization strategies and analyzes the
	tradeooffs between them. This strategies include: "vertex-centric push-based",
	"vertex-centric pull-based", "edge-centric", and linear algebraic implementation
	in addition to the original implementation, which was the vertex-centric
	push-based frontier-based implementation.
	The classification of these alternatives isn't unique to BFS but applies to
	parallelizing graph algorithms in general.

	16_chap "Deep Learning":
	Provides a comprehensive yet intuitive theoretical backgroung for understanding
	modern neural networking.

	17_chap "":
	
	18_chap "":
	
	19_chap "Parallel Programming and Computational Thinking":
	Serves as concluding chapter for parts I and II. The discussion of problem
	decomposition is particularly expanded to introduce the generalizations of
	'output-centric decomposition' and 'input-centric decomposition' to discuss
	the tradeoffs between them, using many examples.

Part 4 "Advances practices"

	20_chap "":

	21_chap "CUDA Dynamic Parallelism":
	Exposes application examples with the other programming details discudes more
	briefly while referring interested readers to the CUDA programming guide.

	22_chap "":

